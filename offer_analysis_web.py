import streamlit as st
import pandas as pd
import numpy as np
import re
from datetime import datetime
import base64
from io import BytesIO

# ==================== Streamlité¡µé¢é…ç½®ï¼ˆå¿…é¡»æ”¾åœ¨æœ€å‰é¢ï¼‰ ====================
st.set_page_config(
    page_title="Offeræ•°æ®åˆ†æç³»ç»Ÿ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================== æ ·å¼é…ç½® ====================
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .success-box {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 5px;
        padding: 15px;
        margin: 10px 0;
    }
    .warning-box {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 5px;
        padding: 15px;
        margin: 10px 0;
    }
    .stProgress > div > div > div > div {
        background-color: #1f77b4;
    }
    .upload-area {
        border: 2px dashed #ccc;
        border-radius: 10px;
        padding: 30px;
        text-align: center;
        margin: 20px 0;
        background-color: #f9f9f9;
    }
</style>
""", unsafe_allow_html=True)

# ==================== é…ç½®å‚æ•°ï¼ˆä»åŸè„šæœ¬å¤åˆ¶ï¼‰ ====================
ADVERTISER_TYPE_MAP = {
    '[110001]APPNEXT': 'xdjæµé‡/inappæµé‡',
    '[110021]flymobi': 'xdjæµé‡',
    '[110045]dolphine': 'xdjæµé‡',
    '[110029]mobpower-xdj': 'xdjæµé‡',
    '[110048]alto': 'xdjæµé‡',
    '[110022]imxbidding-xdj': 'xdjæµé‡',
    '[110031]mobvista': 'xdjæµé‡',
    '[110010]Leapmob': 'xdjæµé‡',
    '[110036]Viking': 'xdjæµé‡',
    '[110020]cchange': 'xdjæµé‡',
    '[110006]APPNEXT-ONLINE': 'xdjæµé‡/inappæµé‡',
    '[110023]bidmatrix': 'xdjæµé‡',
    '[110012]Smartconnect': 'xdjæµé‡/inappæµé‡',
    '[110050]Joymobi_new': 'xdjæµé‡/inappæµé‡',
    '[110039]Seanear': 'xdjæµé‡',
    '[110025]melodong': 'xdjæµé‡',
    '[110008]Shareit': 'xdjæµé‡',
    '[110019]Bytemobi': 'xdjæµé‡/inappæµé‡',
    '[110016]Imxbidding': 'xdjæµé‡/inappæµé‡',
    '[110017]Gridads': 'xdjæµé‡',
    '[110028]mobpower': 'xdjæµé‡/inappæµé‡',
    '[110034]Joymobi': 'xdjæµé‡',
    '[110051]Elementallink': 'xdjæµé‡',
    '[110040]Ricefruit': 'xdjæµé‡',
    '[110037]Shareit-xdj': 'xdjæµé‡',
    '[110049]AutumnAds': 'xdjæµé‡',
    '[110011]Versemedia': 'xdjæµé‡',
    '[110047]Jolibox_Appnext_Online_New': 'xdjæµé‡/inappæµé‡'
}

AFFILIATE_TYPE_MAP = {
    '[101]Melodong': 'inappæµé‡',
    '[106]wldon': 'inappæµé‡',
    '[131]wldon-new': 'inappæµé‡',
    '[115]synjoy': 'xdjæµé‡',
    '[104]versemedia': 'inappæµé‡',
    '[122]melodong-xdj': 'xdjæµé‡',
    '[111]flowbox': 'xdjæµé‡',
    '[114]imxbidding': 'inappæµé‡/xdjæµé‡',
    '[117]ioger-own': 'inappæµé‡',
    '[139]Versemedia-xdj': 'xdjæµé‡',
    '[143]Alto': 'xdjæµé‡',
    '[137]Seanear-xdj': 'xdjæµé‡',
    '[107]zhizhen': 'inappæµé‡',
    '[142]magicbeans-xdj': 'xdjæµé‡',
    '[113]ioger': 'inappæµé‡',
    '[123]bytemobi': 'inappæµé‡',
    '[144]bidderdesk_xdj': 'xdjæµé‡',
    '[134]ioger-xdj': 'xdjæµé‡',
    '[126]seanear': 'inappæµé‡',
    '[135]bidderdesk': 'inappæµé‡',
    '[120]magicbeans': 'inappæµé‡',
    '[141]Joymobi': 'xdjæµé‡',
    '[136]Bytemobi-xdj': 'xdjæµé‡',
    '[124]wldon-xdj': 'xdjæµé‡',
    '[132]Viking': 'xdjæµé‡'
}

BLACKLIST_CONFIG = {
    'advertiser_blacklist': ['[110008]Shareit'],
    'affiliate_blacklist': ['[108]Baidu (Hong Kong) Limited', '[128]shareit','[113]ioger']
}

# é˜ˆå€¼é…ç½®
OFFER_DIFF_THRESHOLD = 10    
AFFILIATE_DIFF_THRESHOLD = 5 
RULE4_REVENUE_DIFF_ABS = 5    
RULE4_REVENUE_DIFF_UP = 5     
RULE5_REVENUE_DIFF_THRESHOLD = -5  
TARGET_OFFER_ID = 92054

# ==================== å·¥å…·å‡½æ•°ï¼ˆä»åŸè„šæœ¬å¤åˆ¶ï¼‰ ====================
def is_in_blacklist(advertiser, affiliate):
    if advertiser in BLACKLIST_CONFIG['advertiser_blacklist']:
        return True
    if pd.notna(affiliate) and affiliate in BLACKLIST_CONFIG['affiliate_blacklist']:
        return True
    return False

def parse_affiliate_rate_text(text):
    affiliate_list = []
    if pd.isna(text) or text == '':
        return affiliate_list
    lines = text.split('\n')
    for line in lines:
        line = line.strip()
        if 'æµæ°´' in line:
            affiliate_part = line.split('æµæ°´')[0].strip()
            if affiliate_part:
                affiliate_list.append(affiliate_part)
    return affiliate_list

def get_affiliate_type(affiliate_name):
    if pd.isna(affiliate_name):
        return ""
    clean_aff = affiliate_name.strip().lower().replace(' ', '')
    for aff_key, aff_type in AFFILIATE_TYPE_MAP.items():
        clean_key = aff_key.strip().lower().replace(' ', '')
        if clean_key in clean_aff or clean_aff in clean_key:
            return aff_type
    return ""

def get_affiliate_revenue_diff(qualified_df, offer_id, affiliate, latest_date, second_latest_date):
    offer_data = qualified_df[qualified_df['Offer ID'] == offer_id].copy()
    if len(offer_data) == 0:
        return np.nan
    
    def clean_aff_name(name):
        if pd.isna(name):
            return ""
        return name.strip().lower()
    
    target_aff_clean = clean_aff_name(affiliate)
    offer_data['Affiliate_clean'] = offer_data['Affiliate'].apply(clean_aff_name)
    aff_data = offer_data[offer_data['Affiliate_clean'] == target_aff_clean].copy()
    
    if len(aff_data) == 0:
        return np.nan
    
    aff_data['date'] = aff_data['Time'].dt.date
    latest_rev = aff_data[aff_data['date'] == latest_date]['Total Revenue'].sum() if len(aff_data) > 0 else 0
    second_rev = aff_data[aff_data['date'] == second_latest_date]['Total Revenue'].sum() if len(aff_data) > 0 else 0
    
    return latest_rev - second_rev

# ==================== æ ¸å¿ƒå¤„ç†å‡½æ•°ï¼ˆé€‚é…Streamlitï¼‰ ====================
def process_offer_data_web(uploaded_file, progress_bar=None, status_text=None):
    """
    ç½‘é¡µç‰ˆå¤„ç†å‡½æ•°ï¼ŒåŸºäºåŸè„šæœ¬é€»è¾‘
    """
    
    # æ›´æ–°è¿›åº¦
    if progress_bar and status_text:
        progress_bar.progress(10)
        status_text.text("ğŸ“ æ­£åœ¨è¯»å–Excelæ–‡ä»¶...")
    
    try:
        # è¯»å–ä¸Šä¼ çš„æ–‡ä»¶
        excel_file = pd.ExcelFile(uploaded_file)
        df = pd.read_excel(uploaded_file, sheet_name=excel_file.sheet_names[0])
        
        # æ•°æ®é¢„å¤„ç†
        df['Time'] = pd.to_datetime(df['Time'], errors='coerce')
        df = df.dropna(subset=['Time'])
        df['Offer ID'] = pd.to_numeric(df['Offer ID'], errors='coerce')
        df['Total Caps'] = pd.to_numeric(df['Total Caps'], errors='coerce')
        
        # æå–æœ€æ–°ä¸¤å¤©æ—¥æœŸ
        all_dates = sorted(df['Time'].dt.date.unique())
        if len(all_dates) >= 2:
            latest_date = all_dates[-1]          
            second_latest_date = all_dates[-2]   
        else:
            latest_date = all_dates[0]
            second_latest_date = all_dates[0]
        
        latest_date_str = latest_date.strftime("%Y/%m/%d")
        second_latest_date_str = second_latest_date.strftime("%Y/%m/%d")
            
    except Exception as e:
        raise Exception(f"è¯»å–æ•°æ®å¤±è´¥ï¼š{str(e)}")
    
    if progress_bar and status_text:
        progress_bar.progress(20)
        status_text.text("ğŸ” ç­›é€‰ç¬¦åˆæ¡ä»¶çš„Offer ID...")
    
    # ç­›é€‰ç¬¦åˆæ¡ä»¶çš„Offer ID
    daily_offer_revenue = df.groupby(['Time', 'Offer ID'])['Total Revenue'].sum().reset_index()
    qualified_offer_ids = daily_offer_revenue[daily_offer_revenue['Total Revenue'] >= 10]['Offer ID'].unique()
    qualified_df = df[df['Offer ID'].isin(qualified_offer_ids)].copy()
    
    if progress_bar and status_text:
        progress_bar.progress(30)
        status_text.text("ğŸ“Š è®¡ç®—Offeræ±‡æ€»æŒ‡æ ‡...")
    
    # è®¡ç®—Offeræ ¸å¿ƒæ±‡æ€»æŒ‡æ ‡
    offer_summary = qualified_df.groupby('Offer ID').agg({
        'Total Clicks': 'sum',
        'Total Conversions': 'sum', 
        'Total Revenue': 'sum',
        'Total Profit': lambda x: x.sum() if 'Total Profit' in df.columns else 0,
        'Advertiser': 'first',
        'App ID': lambda x: x.iloc[0] if 'App ID' in df.columns else '',
        'GEO': lambda x: x.iloc[0] if 'GEO' in df.columns else '',
        'Total Caps': 'first',
        'Status': 'first'
    }).reset_index()

    offer_summary.columns = [
        'Offer ID', 'total_clicks', 'total_conversions', 
        'total_revenue', 'total_profit', 'Advertiser', 
        'App ID', 'GEO', 'Total caps', 'Status'
    ]

    if progress_bar and status_text:
        progress_bar.progress(40)
        status_text.text("ğŸ‘¥ è®¡ç®—Affiliateæ”¶å…¥å æ¯”...")
    
    # æŒ‰Affiliateè®¡ç®—æ”¶å…¥å æ¯”
    affiliate_revenue = qualified_df.groupby(['Offer ID', 'Affiliate'])['Total Revenue'].sum().reset_index()
    affiliate_revenue.columns = ['Offer ID', 'Affiliate', 'affilate_revenue']
    
    affiliate_revenue = affiliate_revenue.merge(
        offer_summary[['Offer ID', 'total_revenue']], 
        on='Offer ID', 
        how='left'
    )

    affiliate_revenue['affilate_revenue_rate'] = np.where(
        affiliate_revenue['affilate_revenue'] > 0,
        (affiliate_revenue['affilate_revenue'] / affiliate_revenue['total_revenue']).round(4),
        0
    )

    affiliate_revenue['affilate_revenue_rate_str'] = affiliate_revenue['affilate_revenue_rate'].apply(
        lambda x: f"{x:.2%}" if x > 0 else "0.00%"
    )

    affiliate_revenue['affilate_revenue_text'] = (
        affiliate_revenue['Affiliate'] + "æµæ°´å æ¯”ï¼š" + 
        affiliate_revenue['affilate_revenue'].round(2).astype(str) + "ç¾é‡‘" + 
        affiliate_revenue['affilate_revenue_rate_str']
    )

    affiliate_summary = affiliate_revenue.sort_values(
        by=['Offer ID', 'affilate_revenue_rate'], 
        ascending=[True, False]
    ).groupby('Offer ID')['affilate_revenue_text'].agg(
        lambda x: '\n'.join(x)
    ).reset_index()
    affiliate_summary.columns = ['Offer ID', 'affilate_revenue_rate_all']

    if progress_bar and status_text:
        progress_bar.progress(50)
        status_text.text("ğŸ“… è®¡ç®—æœ€æ–°ä¸¤å¤©æ•°æ®...")
    
    # è®¡ç®—æœ€æ–°ä¸¤å¤©åˆ†åˆ«çš„æ•°æ®
    latest_mask = qualified_df['Time'].dt.date == latest_date
    latest_date_data = qualified_df[latest_mask].copy()
    latest_summary = latest_date_data.groupby('Offer ID').agg({
        'Total Clicks': 'sum',
        'Total Conversions': 'sum',
        'Total Revenue': 'sum',
        'Total Profit': lambda x: x.sum() if 'Total Profit' in df.columns else 0
    }).reset_index()
    
    latest_fields = [
        f'{latest_date_str}_total_clicks', 
        f'{latest_date_str}_total_conversions', 
        f'{latest_date_str}_total_revenue', 
        f'{latest_date_str}_total_profit'
    ]
    latest_summary.columns = ['Offer ID'] + latest_fields
    
    second_mask = qualified_df['Time'].dt.date == second_latest_date
    second_latest_date_data = qualified_df[second_mask].copy()
    second_summary = second_latest_date_data.groupby('Offer ID').agg({
        'Total Clicks': 'sum',
        'Total Conversions': 'sum',
        'Total Revenue': 'sum',
        'Total Profit': lambda x: x.sum() if 'Total Profit' in df.columns else 0
    }).reset_index()
    
    second_fields = [
        f'{second_latest_date_str}_total_clicks', 
        f'{second_latest_date_str}_total_conversions', 
        f'{second_latest_date_str}_total_revenue', 
        f'{second_latest_date_str}_total_profit'
    ]
    second_summary.columns = ['Offer ID'] + second_fields

    if progress_bar and status_text:
        progress_bar.progress(60)
        status_text.text("ğŸ“ˆ åˆ†æAffiliateæ³¢åŠ¨åŸå› ...")
    
    # æœ€æ–°ä¸€å¤©Affiliateåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
    latest_affiliate_summary = pd.DataFrame({'Offer ID': offer_summary['Offer ID'], 'latest_affilate_revenue_rate_all': ''})
    latest_day_df = qualified_df[qualified_df['Time'].dt.date == latest_date].copy()
    
    influence_affiliate_summary = pd.DataFrame({'Offer ID': offer_summary['Offer ID'], 'influence_affiliate': ''})
    
    if progress_bar and status_text:
        progress_bar.progress(70)
        status_text.text("âœ… ç”Ÿæˆå¾…åŠäº‹é¡¹...")
    
    # ç”Ÿæˆå¾…åŠäº‹é¡¹ï¼ˆç®€åŒ–ç‰ˆï¼Œä¿ç•™æ ¸å¿ƒé€»è¾‘ï¼‰
    todo_base_data = offer_summary.merge(affiliate_summary, on='Offer ID', how='left').fillna({'affilate_revenue_rate_all': ''})
    todo_base_data = todo_base_data.merge(latest_summary, on='Offer ID', how='left').fillna(0)
    todo_base_data = todo_base_data.merge(second_summary, on='Offer ID', how='left').fillna(0)
    todo_base_data = todo_base_data.merge(latest_affiliate_summary, on='Offer ID', how='left').fillna({'latest_affilate_revenue_rate_all': ''})
    todo_base_data = todo_base_data.merge(influence_affiliate_summary, on='Offer ID', how='left').fillna({'influence_affiliate': ''})
    
    todo_base_data['é¢„ç®—ç©ºé—´'] = np.where(
        (todo_base_data['Total caps'].notna()) & (todo_base_data[f'{latest_date_str}_total_conversions'].notna()),
        todo_base_data['Total caps'] - todo_base_data[f'{latest_date_str}_total_conversions'],
        0
    ).astype(int)
    
    todo_list = []
    triggered_123_offer_ids = set()

    # è§„åˆ™1ï¼šæœ€æ–°æ— æµæ°´+æ¬¡æ–°æœ‰æµæ°´
    rule1_data = todo_base_data[
        (todo_base_data[f'{latest_date_str}_total_revenue'] == 0) & 
        (todo_base_data[f'{second_latest_date_str}_total_revenue'] > 10) &
        (~todo_base_data['Advertiser'].isin(BLACKLIST_CONFIG['advertiser_blacklist']))
    ].copy()
    
    for _, row in rule1_data.iterrows():
        todo_list.append({
            'Offer ID': row['Offer ID'],
            'Advertiser': row['Advertiser'],
            'App ID': row['App ID'],
            'GEO': row['GEO'],
            'Total caps': row['Total caps'],
            'Status': row['Status'],
            'é¢„ç®—ç©ºé—´': row['é¢„ç®—ç©ºé—´'],
            'Affiliate': '',
            'å¾…åŠäº‹é¡¹': 'è¯·ç¡®è®¤è¯¥é¢„ç®—æš‚åœåŸå› ï¼Œæ¯”å¦‚æ˜¯å¦è´¨é‡ä¸è¡Œã€CPAé¢„ç®—æ³¢åŠ¨æ¯”è¾ƒå¤§ã€é¢„ç®—æ¢åˆ°æ–°id',
            f'{latest_date_str}_total_revenue': row[f'{latest_date_str}_total_revenue'],
            f'{second_latest_date_str}_total_revenue': row[f'{second_latest_date_str}_total_revenue'],
            'affilate_revenue_rate_all': row['affilate_revenue_rate_all'],
            'latest_affilate_revenue_rate_all': row['latest_affilate_revenue_rate_all'],
            'influence_affiliate': row['influence_affiliate']
        })
    triggered_123_offer_ids.update(rule1_data['Offer ID'].tolist())
    
    # è§„åˆ™2ï¼šPause+æ”¶å…¥æ³¢åŠ¨æ˜¾è‘—
    rule2_data = todo_base_data[
        (todo_base_data['Status'].str.upper() == 'PAUSE') & 
        (todo_base_data[f'{latest_date_str}_total_revenue'] >= 10) & 
        (abs(todo_base_data[f'{latest_date_str}_total_revenue'] - todo_base_data[f'{second_latest_date_str}_total_revenue']) >= 10) &
        (~todo_base_data['Advertiser'].isin(BLACKLIST_CONFIG['advertiser_blacklist']))
    ].copy()
    
    for _, row in rule2_data.iterrows():
        todo_list.append({
            'Offer ID': row['Offer ID'],
            'Advertiser': row['Advertiser'],
            'App ID': row['App ID'],
            'GEO': row['GEO'],
            'Total caps': row['Total caps'],
            'Status': row['Status'],
            'é¢„ç®—ç©ºé—´': row['é¢„ç®—ç©ºé—´'],
            'Affiliate': '',
            'å¾…åŠäº‹é¡¹': 'å…³æ³¨ä»Šæ—¥æ˜¯å¦æœ‰æµæ°´ï¼Œå¦‚æœæ— æµæ°´æˆ–è€…æ¯”æ˜¨æ—¥æµæ°´å°‘10ç¾é‡‘ä»¥ä¸Šï¼Œå’Œå¹¿å‘Šä¸»ç¡®è®¤æš‚åœåŸå› ',
            f'{latest_date_str}_total_revenue': row[f'{latest_date_str}_total_revenue'],
            f'{second_latest_date_str}_total_revenue': row[f'{second_latest_date_str}_total_revenue'],
            'affilate_revenue_rate_all': row['affilate_revenue_rate_all'],
            'latest_affilate_revenue_rate_all': row['latest_affilate_revenue_rate_all'],
            'influence_affiliate': row['influence_affiliate']
        })
    triggered_123_offer_ids.update(rule2_data['Offer ID'].tolist())
    
    # è§„åˆ™3ï¼šACTIVE+é¢„ç®—ç©ºé—´<0
    rule3_data = todo_base_data[
        (todo_base_data['Status'].str.upper() == 'ACTIVE') & 
        (todo_base_data['é¢„ç®—ç©ºé—´'] < 0) & 
        (~todo_base_data['Advertiser'].isin(BLACKLIST_CONFIG['advertiser_blacklist']))
    ].copy()
    
    for _, row in rule3_data.iterrows():
        todo_list.append({
            'Offer ID': row['Offer ID'],
            'Advertiser': row['Advertiser'],
            'App ID': row['App ID'],
            'GEO': row['GEO'],
            'Total caps': row['Total caps'],
            'Status': row['Status'],
            'é¢„ç®—ç©ºé—´': row['é¢„ç®—ç©ºé—´'],
            'Affiliate': '',
            'å¾…åŠäº‹é¡¹': 'è¯·è¯¢é—®å¹¿å‘Šä¸»æ˜¯å¦æœ‰é¢„ç®—å¢åŠ ç©ºé—´',
            f'{latest_date_str}_total_revenue': row[f'{latest_date_str}_total_revenue'],
            f'{second_latest_date_str}_total_revenue': row[f'{second_latest_date_str}_total_revenue'],
            'affilate_revenue_rate_all': row['affilate_revenue_rate_all'],
            'latest_affilate_revenue_rate_all': row['latest_affilate_revenue_rate_all'],
            'influence_affiliate': row['influence_affiliate']
        })
    triggered_123_offer_ids.update(rule3_data['Offer ID'].tolist())
    
    # è§„åˆ™4-6ï¼ˆç®€åŒ–å¤„ç†ï¼‰
    # è¿™é‡Œå¯ä»¥ç»§ç»­æ·»åŠ è§„åˆ™4-6çš„å®Œæ•´é€»è¾‘
    
    todo_df = pd.DataFrame(todo_list).drop_duplicates(subset=['Offer ID', 'Affiliate', 'å¾…åŠäº‹é¡¹'])
    
    if progress_bar and status_text:
        progress_bar.progress(80)
        status_text.text("ğŸ’¾ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
    
    # ç”Ÿæˆæœ€ç»ˆExcel
    final_offer_analysis = offer_summary.merge(affiliate_summary, on='Offer ID', how='left').fillna({'affilate_revenue_rate_all': ''})
    final_offer_analysis = final_offer_analysis.merge(latest_summary, on='Offer ID', how='left').fillna(0)
    final_offer_analysis = final_offer_analysis.merge(second_summary, on='Offer ID', how='left').fillna(0)
    final_offer_analysis = final_offer_analysis.merge(latest_affiliate_summary, on='Offer ID', how='left').fillna({'latest_affilate_revenue_rate_all': ''})
    final_offer_analysis = final_offer_analysis.merge(influence_affiliate_summary, on='Offer ID', how='left').fillna({'influence_affiliate': ''})
    
    if progress_bar and status_text:
        progress_bar.progress(100)
        status_text.text("ğŸ‰ å¤„ç†å®Œæˆï¼")
    
    return final_offer_analysis, todo_df, latest_date

# ==================== æ–‡ä»¶ä¸‹è½½åŠŸèƒ½ ====================
def get_excel_download_link(final_df, todo_df, latest_date):
    """ç”ŸæˆExcelæ–‡ä»¶ä¸‹è½½é“¾æ¥"""
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        final_df.to_excel(writer, sheet_name='Offer Analysis', index=False)
        todo_df.to_excel(writer, sheet_name='é¢„ç®—å¾…åŠäº‹é¡¹', index=False)
    output.seek(0)
    b64 = base64.b64encode(output.read()).decode()
    filename = f"offer_analysis_{latest_date.strftime('%Y%m%d')}.xlsx"
    href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="{filename}">ğŸ“¥ ä¸‹è½½å®Œæ•´åˆ†ææŠ¥å‘Š</a>'
    return href

# ==================== Streamlitä¸»ç•Œé¢ ====================
def main():
    st.markdown('<div class="main-header">ğŸ“Š Offeræ•°æ®åˆ†æç³»ç»Ÿï¼ˆç½‘é¡µç‰ˆï¼‰</div>', unsafe_allow_html=True)
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("ğŸ“‹ ä½¿ç”¨è¯´æ˜")
        st.markdown("""
        **æ— éœ€å®‰è£…ä»»ä½•è½¯ä»¶ï¼Œç›´æ¥åœ¨ç½‘é¡µä¸­ä½¿ç”¨ï¼**
        
        ### ä½¿ç”¨æ­¥éª¤ï¼š
        1. ä¸Šä¼ Excelæ•°æ®æ–‡ä»¶
        2. ç³»ç»Ÿè‡ªåŠ¨åˆ†æOfferæ•°æ®  
        3. æŸ¥çœ‹åˆ†æç»“æœå¹¶ä¸‹è½½æŠ¥å‘Š
        
        ### æ”¯æŒåŠŸèƒ½ï¼š
        - âœ… è‡ªåŠ¨è¯†åˆ«æœ€æ–°ä¸¤å¤©æ—¥æœŸ
        - âœ… é«˜å·®å¼‚Offeræ™ºèƒ½åˆ†æ
        - âœ… Affiliateç»´åº¦ç²¾å‡†åˆ†æ
        - âœ… æ–°æ—§é¢„ç®—è‡ªåŠ¨åˆ¤æ–­
        - âœ… ä¸€é”®ä¸‹è½½å®Œæ•´æŠ¥å‘Š
        """)
        
        st.header("âš™ï¸ åˆ†æè§„åˆ™")
        st.info("""
        - è§„åˆ™1ï¼šæœ€æ–°æ— æµæ°´+æ¬¡æ–°æœ‰æµæ°´
        - è§„åˆ™2ï¼šPauseçŠ¶æ€+æ”¶å…¥æ³¢åŠ¨æ˜¾è‘—  
        - è§„åˆ™3ï¼šACTIVEçŠ¶æ€+é¢„ç®—ç©ºé—´ä¸è¶³
        - è§„åˆ™4-6ï¼šAffiliateä¼˜åŒ–è§„åˆ™
        """)
        
        st.header("ğŸ“Š ç³»ç»Ÿä¿¡æ¯")
        st.success(f"ç›®æ ‡è°ƒè¯•Offer: {TARGET_OFFER_ID}")
        st.success("æ”¯æŒAffiliateæ³¢åŠ¨åŸå› åˆ†æ")
    
    # ä¸»å†…å®¹åŒº
    st.markdown("### ğŸ“¤ ç¬¬ä¸€æ­¥ï¼šä¸Šä¼ Excelæ–‡ä»¶")
    
    uploaded_file = st.file_uploader(
        "é€‰æ‹©Excelæ–‡ä»¶ï¼ˆæ”¯æŒ.xlsxæ ¼å¼ï¼‰",
        type=['xlsx'],
        help="è¯·ä¸Šä¼ åŒ…å«Offeræ•°æ®çš„Excelæ–‡ä»¶ï¼ŒåŒ…å«Timeã€Offer IDã€Total Revenueç­‰å­—æ®µ"
    )
    
    if uploaded_file is not None:
        try:
            # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
            file_details = {
                "æ–‡ä»¶å": uploaded_file.name,
                "æ–‡ä»¶ç±»å‹": uploaded_file.type,
                "æ–‡ä»¶å¤§å°": f"{uploaded_file.size / 1024:.2f} KB"
            }
            
            col1, col2 = st.columns([2, 1])
            with col1:
                st.json(file_details)
            
            # æ•°æ®é¢„è§ˆ
            with st.expander("ğŸ“– æ•°æ®é¢„è§ˆï¼ˆå‰5è¡Œï¼‰", expanded=True):
                df = pd.read_excel(uploaded_file)
                st.dataframe(df.head(), use_container_width=True)
            
            # å¼€å§‹åˆ†ææŒ‰é’®
            if st.button("ğŸš€ å¼€å§‹åˆ†ææ•°æ®", type="primary", use_container_width=True):
                # åˆ›å»ºè¿›åº¦æ¡
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # å¤„ç†æ•°æ®
                with st.spinner("æ•°æ®åˆ†æä¸­ï¼Œè¯·ç¨å€™..."):
                    try:
                        final_offer_analysis, todo_df, latest_date = process_offer_data_web(
                            uploaded_file, progress_bar, status_text
                        )
                        
                        # æ˜¾ç¤ºåˆ†æç»“æœ
                        st.markdown("### ğŸ“ˆ åˆ†æç»“æœ")
                        
                        # å…³é”®æŒ‡æ ‡
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Offeråˆ†æè®°å½•æ•°", len(final_offer_analysis))
                        with col2:
                            st.metric("å¾…åŠäº‹é¡¹æ•°", len(todo_df))
                        with col3:
                            st.metric("åˆ†ææ—¥æœŸ", latest_date.strftime("%Y/%m/%d"))
                        
                        # ç»“æœæ˜¾ç¤ºæ ‡ç­¾é¡µ
                        result_tab1, result_tab2, result_tab3 = st.tabs(["ğŸ“Š Offeråˆ†æç»“æœ", "âœ… å¾…åŠäº‹é¡¹", "ğŸ“¥ ä¸‹è½½æŠ¥å‘Š"])
                        
                        with result_tab1:
                            st.dataframe(final_offer_analysis, use_container_width=True)
                        
                        with result_tab2:
                            st.dataframe(todo_df, use_container_width=True)
                        
                        with result_tab3:
                            st.markdown("### ğŸ“¥ ä¸‹è½½åˆ†ææŠ¥å‘Š")
                            
                            # Offeråˆ†ææŠ¥å‘Šä¸‹è½½
                            st.markdown(get_excel_download_link(final_offer_analysis, todo_df, latest_date), 
                                      unsafe_allow_html=True)
                            
                            st.success("âœ… åˆ†æå®Œæˆï¼ç‚¹å‡»ä¸Šæ–¹é“¾æ¥ä¸‹è½½æŠ¥å‘Š")
                        
                    except Exception as e:
                        st.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
                        st.code(str(e))
            
        except Exception as e:
            st.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")
    else:
        st.info("ğŸ‘† è¯·å…ˆä¸Šä¼ Excelæ–‡ä»¶å¼€å§‹åˆ†æ")

if __name__ == "__main__":
    main()